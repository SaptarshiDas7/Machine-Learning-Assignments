{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG-16.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnRBcZqK7dYh",
        "outputId": "64003cef-1596-4251-c8f1-3e075cee9b27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfvFp9BnOIK4",
        "outputId": "fbcc52d0-04d4-4fc1-cde0-aca4e08866a3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import skimage.transform\n",
        "from __future__ import print_function\n",
        "\n",
        "!pip install keras_applications\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras import backend as K\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.utils.layer_utils import get_source_inputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3dyuQd7OQoN"
      },
      "source": [
        "def load_preprocess_training_batch(X_train):\n",
        "    \n",
        "    new = []\n",
        "    \n",
        "    for item in X_train:\n",
        "        tmpFeature = skimage.transform.resize(item, (224, 224), mode='constant')\n",
        "        new.append(tmpFeature)\n",
        "\n",
        "    return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuXbe2XwOfZD"
      },
      "source": [
        "def preprocess_data(X_train):\n",
        "    \n",
        "    for item in X_train:\n",
        "      item = np.expand_dims(item, axis=0)\n",
        "      item = preprocess_input(item)\n",
        "\n",
        "    return X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXk-KfyGOi1Q"
      },
      "source": [
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "\n",
        "def VGG16(include_top=True, weights='imagenet',\n",
        "          input_tensor=None, input_shape=None,\n",
        "          pooling=None,\n",
        "          classes=1000):\n",
        "    \"\"\"Instantiates the VGG16 architecture.\n",
        "\n",
        "    Optionally loads weights pre-trained\n",
        "    on ImageNet. Note that when using TensorFlow,\n",
        "    for best performance you should set\n",
        "    `image_data_format=\"channels_last\"` in your Keras config\n",
        "    at ~/.keras/keras.json.\n",
        "\n",
        "    The model and the weights are compatible with both\n",
        "    TensorFlow and Theano. The data format\n",
        "    convention used by the model is the one\n",
        "    specified in your Keras config file.\n",
        "\n",
        "    # Arguments\n",
        "        include_top: whether to include the 3 fully-connected\n",
        "            layers at the top of the network.\n",
        "        weights: one of `None` (random initialization)\n",
        "            or \"imagenet\" (pre-training on ImageNet).\n",
        "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
        "            to use as image input for the model.\n",
        "        input_shape: optional shape tuple, only to be specified\n",
        "            if `include_top` is False (otherwise the input shape\n",
        "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
        "            or `(3, 224, 244)` (with `channels_first` data format).\n",
        "            It should have exactly 3 inputs channels,\n",
        "            and width and height should be no smaller than 48.\n",
        "            E.g. `(200, 200, 3)` would be one valid value.\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model will be\n",
        "                the 4D tensor output of the\n",
        "                last convolutional layer.\n",
        "            - `avg` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional layer, and thus\n",
        "                the output of the model will be a 2D tensor.\n",
        "            - `max` means that global max pooling will\n",
        "                be applied.\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is True, and\n",
        "            if no `weights` argument is specified.\n",
        "\n",
        "    # Returns\n",
        "        A Keras model instance.\n",
        "\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `weights`,\n",
        "            or invalid input shape.\n",
        "    \"\"\"\n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `imagenet` '\n",
        "                         '(pre-training on ImageNet).')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=224,\n",
        "                                      min_size=48,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "    if include_top:\n",
        "        # Classification block\n",
        "        x = Flatten(name='flatten')(x)\n",
        "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='vgg16')\n",
        "\n",
        "    # load weights\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                                    WEIGHTS_PATH,\n",
        "                                    cache_subdir='models')\n",
        "        else:\n",
        "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                                    WEIGHTS_PATH_NO_TOP,\n",
        "                                    cache_subdir='models')\n",
        "        model.load_weights(weights_path)\n",
        "        if K.backend() == 'theano':\n",
        "            layer_utils.convert_all_kernels_in_model(model)\n",
        "\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            if include_top:\n",
        "                maxpool = model.get_layer(name='block5_pool')\n",
        "                shape = maxpool.output_shape[1:]\n",
        "                dense = model.get_layer(name='fc1')\n",
        "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
        "\n",
        "            if K.backend() == 'tensorflow':\n",
        "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "                              'are using the Theano '\n",
        "                              'image data format convention '\n",
        "                              '(`image_data_format=\"channels_first\"`). '\n",
        "                              'For best performance, set '\n",
        "                              '`image_data_format=\"channels_last\"` in '\n",
        "                              'your Keras config '\n",
        "                              'at ~/.keras/keras.json.')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdhFYfBmPC0P"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import skimage.transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULzr-rQ66AJR"
      },
      "source": [
        "# CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn7kLBMMOX6S"
      },
      "source": [
        "\n",
        "(X_train, y_train) , (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "X_train = X_train[0:2000]\n",
        "y_train = y_train[0:2000]\n",
        "X_test = X_test[0:2000]\n",
        "y_test = y_test[0:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "curtp8znOZ3M"
      },
      "source": [
        "X_train_resized = load_preprocess_training_batch(X_train)\n",
        "X_test_resized = load_preprocess_training_batch(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snipLfl6OcNq"
      },
      "source": [
        "X_train_resized = np.array(X_train_resized)\n",
        "X_test_resized = np.array(X_test_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juA1DOHUOd8m"
      },
      "source": [
        "X_train_resized = X_train_resized / 255\n",
        "X_test_resized = X_test_resized / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_FYQMDOOhNZ"
      },
      "source": [
        "X_train_resized = preprocess_data(X_train_resized)\n",
        "X_test_resized = preprocess_data(X_test_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX6y0ZbtQXjT",
        "outputId": "a3b2ebfe-a4de-4ed7-e6de-7252963bad8a"
      },
      "source": [
        "model = VGG16(include_top=True, weights='imagenet')\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_resized, y_train, epochs=5)\n",
        "\n",
        "# img_path = 'a.jpg'\n",
        "# img = image.load_img(img_path, target_size=(224, 224))\n",
        "# x = image.img_to_array(img)\n",
        "# x = np.expand_dims(x, axis=0)\n",
        "# x = preprocess_input(x)\n",
        "# print('Input image shape:', x.shape)\n",
        "\n",
        "# preds = model.predict(x)\n",
        "# print('Predicted:', decode_predictions(preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 [==============================] - 97s 855ms/step - loss: nan - accuracy: 0.0985\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 47s 743ms/step - loss: nan - accuracy: 0.1010\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 47s 745ms/step - loss: nan - accuracy: 0.1010\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 47s 744ms/step - loss: nan - accuracy: 0.1010\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 47s 744ms/step - loss: nan - accuracy: 0.1010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C11RI9kJRRXk",
        "outputId": "f99b62d3-7ce5-4e39-d793-f5d6d6fb5b80"
      },
      "source": [
        "model.evaluate(X_test_resized, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 14s 225ms/step - loss: nan - accuracy: 0.0980\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.09799999743700027]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAxYncGG6XEo"
      },
      "source": [
        "# MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB_aT-n8XRiA"
      },
      "source": [
        "(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "X_train = X_train[0:2000]\n",
        "y_train = y_train[0:2000]\n",
        "X_test = X_test[0:2000]\n",
        "y_test = y_test[0:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeTasC3aXW2M"
      },
      "source": [
        "X_train_resized = load_preprocess_training_batch(X_train)\n",
        "X_test_resized = load_preprocess_training_batch(X_test)\n",
        "\n",
        "X_train_resized = np.array(X_train_resized)\n",
        "X_test_resized = np.array(X_test_resized)\n",
        "\n",
        "X_train_resized = X_train_resized / 255.0\n",
        "X_test_resized = X_test_resized / 255.0\n",
        "\n",
        "X_train_resized = preprocess_data(X_train_resized)\n",
        "X_test_resized = preprocess_data(X_test_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh9O7hOkXZNO"
      },
      "source": [
        "import cv2\n",
        "\n",
        "X_train_new = list()\n",
        "\n",
        "for i in range(len(X_train_resized)):\n",
        "  g  = X_train_resized[i]\n",
        "  X_train_new.append(cv2.merge([g,g,g]))\n",
        "\n",
        "X_train_new = np.asarray(X_train_new,dtype=np.float32)\n",
        "\n",
        "X_test_new = list()\n",
        "\n",
        "for i in range(len(X_test_resized)):\n",
        "  g  = X_test_resized[i]\n",
        "  X_test_new.append(cv2.merge([g,g,g]))\n",
        "\n",
        "X_test_new = np.asarray(X_test_new,dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ltJi1haXg8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4909b7-553d-40c6-e5c2-a4620240846d"
      },
      "source": [
        "model = VGG16(include_top=True, weights='imagenet')\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_new, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 [==============================] - 71s 877ms/step - loss: 3.6193 - accuracy: 0.0885\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 48s 764ms/step - loss: 2.5311 - accuracy: 0.0990\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 48s 763ms/step - loss: 2.5246 - accuracy: 0.1105\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 48s 763ms/step - loss: 2.5036 - accuracy: 0.1040\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 48s 763ms/step - loss: 2.4806 - accuracy: 0.0965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ave1bSmuXllc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17057d83-71fd-4b63-ea43-a3be0e61dbd4"
      },
      "source": [
        "model.evaluate(X_test_new, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 15s 233ms/step - loss: 2.6352 - accuracy: 0.1095\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.6351511478424072, 0.10949999839067459]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgnYTZSd7JO4"
      },
      "source": [
        "# SAVEE Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm1uWEQp7O8w"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/SaveeDataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-mIUZLn7xCI"
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "input_length = 16000*5\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "n_mels = 320\n",
        "\n",
        "def preprocess_audio_mel_T(audio, sample_rate=16000, window_size=20, #log_specgram\n",
        "                 step_size=10, eps=1e-10):\n",
        "\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels= n_mels)\n",
        "    mel_db = (librosa.power_to_db(mel_spec, ref=np.max) + 40)/40\n",
        "\n",
        "    return mel_db.T\n",
        "\n",
        "\n",
        "def load_audio_file(file_path, input_length=input_length):\n",
        "  data = librosa.core.load(file_path, sr=16000)[0] #, sr=16000\n",
        "  if len(data)>input_length:\n",
        "    max_offset = len(data)-input_length\n",
        "    \n",
        "    offset = np.random.randint(max_offset)\n",
        "    \n",
        "    data = data[offset:(input_length+offset)]\n",
        "            \n",
        "  else:\n",
        "    if input_length > len(data):\n",
        "      max_offset = input_length - len(data)\n",
        "\n",
        "      offset = np.random.randint(max_offset)\n",
        "    else:\n",
        "      offset = 0\n",
        "    data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
        "    \n",
        "  data = preprocess_audio_mel_T(data)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5WNGb6C8DLw"
      },
      "source": [
        "# Preprocessing the dataset\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "rootDirectory = \"/content/AudioData/\"\n",
        "personNames = [\"DC\",\"JE\",\"JK\",\"KL\"]\n",
        "\n",
        "classes = [\"a\" , \"d\" , \"f\", \"h\", \"n\", \"sa\" , \"su\" ]\n",
        "\n",
        "X = list()\n",
        "y = list()\n",
        "\n",
        "for person in personNames:\n",
        "  directory = os.path.join(rootDirectory,person)\n",
        "  for filename in os.listdir(directory):\n",
        "    filePath = os.path.join(directory, filename)\n",
        "    a = load_audio_file(file_path=filePath)\n",
        "    data = cv2.merge([a,a,a])\n",
        "    # data = np.reshape(data, data.shape + (1,))\n",
        "    if(filename[0:1] in classes):\n",
        "      X.append(data)\n",
        "      y.append(classes.index(filename[0:1]))\n",
        "    elif(filename[0:2] in classes):\n",
        "      X.append(data)\n",
        "      y.append(classes.index(filename[0:2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1-HbMsI9heI"
      },
      "source": [
        "X = np.asarray(X, dtype=np.float32)\n",
        "y = np.asarray(y, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahNdGovV9urg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# dataset preparation\n",
        "\n",
        "from tensorflow.keras import datasets,layers,models\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, train_size= 0.5 ,random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E7avNXV9xx4"
      },
      "source": [
        "X_train_resized = load_preprocess_training_batch(X_train)\n",
        "X_test_resized = load_preprocess_training_batch(X_test)\n",
        "\n",
        "X_train_resized = np.array(X_train_resized)\n",
        "X_test_resized = np.array(X_test_resized)\n",
        "\n",
        "X_train_resized = preprocess_data(X_train_resized)\n",
        "X_test_resized = preprocess_data(X_test_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYdESnC3-Ly4",
        "outputId": "3564a07d-40c5-4314-b10e-385be153a2c5"
      },
      "source": [
        "model = VGG16(include_top=True, weights='imagenet')\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_resized, y_train, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 7s 725ms/step - loss: nan - accuracy: 0.0917\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 6s 708ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 6s 708ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 6s 709ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 6s 709ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 6s 708ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 6s 709ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 6s 710ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 6s 711ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 6s 704ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 6s 708ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 6s 709ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 6s 709ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 6s 703ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 6s 707ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 6s 708ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 6s 705ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 6s 709ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: nan - accuracy: 0.1208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMAnoIgd-6Ta",
        "outputId": "beeb9576-5136-4a45-82dd-7d18a3fc7c9c"
      },
      "source": [
        "model.evaluate(X_test_resized, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 2s 215ms/step - loss: nan - accuracy: 0.1292\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.12916666269302368]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX7ciYAV_Kid"
      },
      "source": [
        "# EmoDb Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmuh7PUL_NTg"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/EmoDB.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAHkoEvm_VKw"
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "input_length = 16000*5\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "n_mels = 320\n",
        "\n",
        "def preprocess_audio_mel_T(audio, sample_rate=16000, window_size=20, #log_specgram\n",
        "                 step_size=10, eps=1e-10):\n",
        "\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels= n_mels)\n",
        "    mel_db = (librosa.power_to_db(mel_spec, ref=np.max) + 40)/40\n",
        "\n",
        "    return mel_db.T\n",
        "\n",
        "\n",
        "def load_audio_file(file_path, input_length=input_length):\n",
        "  data = librosa.core.load(file_path, sr=16000)[0] #, sr=16000\n",
        "  if len(data)>input_length:\n",
        "    max_offset = len(data)-input_length\n",
        "    \n",
        "    offset = np.random.randint(max_offset)\n",
        "    \n",
        "    data = data[offset:(input_length+offset)]\n",
        "            \n",
        "  else:\n",
        "    if input_length > len(data):\n",
        "      max_offset = input_length - len(data)\n",
        "\n",
        "      offset = np.random.randint(max_offset)\n",
        "    else:\n",
        "      offset = 0\n",
        "    data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
        "    \n",
        "  data = preprocess_audio_mel_T(data)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWmn7sOW_Xt7"
      },
      "source": [
        "# Preprocessing the dataset\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "directory = \"/content/wav/\"\n",
        "\n",
        "classes = [\"W\" ,\"L\" ,\"E\" ,\"A\" , \"F\" ,\"T\" ,\"N\" ]\n",
        "\n",
        "X = list()\n",
        "y = list()\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "  filePath = os.path.join(directory, filename)\n",
        "  a = load_audio_file(file_path=filePath)\n",
        "  data = cv2.merge([a,a,a])\n",
        "  if(filename[5:6] in classes):\n",
        "    X.append(data)\n",
        "    y.append(classes.index(filename[5:6]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5lq4gg9_iAo"
      },
      "source": [
        "X = np.asarray(X, dtype=np.float32)\n",
        "y = np.asarray(y, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wTpVUBu_o94"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# dataset preparation\n",
        "\n",
        "from tensorflow.keras import datasets,layers,models\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, train_size= 0.5 ,random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suM4sBd9_vij"
      },
      "source": [
        "X_train_resized = load_preprocess_training_batch(X_train)\n",
        "X_test_resized = load_preprocess_training_batch(X_test)\n",
        "\n",
        "X_train_resized = np.array(X_train_resized)\n",
        "X_test_resized = np.array(X_test_resized)\n",
        "\n",
        "X_train_resized = preprocess_data(X_train_resized)\n",
        "X_test_resized = preprocess_data(X_test_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMuuVQ-c_wF_",
        "outputId": "d9509ee0-4217-44b8-c1ee-1838480a306d"
      },
      "source": [
        "model = VGG16(include_top=True, weights='imagenet')\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_resized, y_train, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "9/9 [==============================] - 13s 1s/step - loss: nan - accuracy: 0.1610\n",
            "Epoch 2/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 3/20\n",
            "9/9 [==============================] - 6s 713ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 4/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 5/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 6/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 7/20\n",
            "9/9 [==============================] - 6s 713ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 8/20\n",
            "9/9 [==============================] - 6s 713ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 9/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 10/20\n",
            "9/9 [==============================] - 6s 711ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 11/20\n",
            "9/9 [==============================] - 6s 715ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 12/20\n",
            "9/9 [==============================] - 6s 710ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 13/20\n",
            "9/9 [==============================] - 6s 711ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 14/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 15/20\n",
            "9/9 [==============================] - 6s 713ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 16/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 17/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 18/20\n",
            "9/9 [==============================] - 6s 711ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 19/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n",
            "Epoch 20/20\n",
            "9/9 [==============================] - 6s 712ms/step - loss: nan - accuracy: 0.2247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea2-wNtq_z4_",
        "outputId": "bac51feb-e324-4a01-ad22-d99dcd5e1e3c"
      },
      "source": [
        "model.evaluate(X_test_resized, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 6s 718ms/step - loss: nan - accuracy: 0.2500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.25]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}